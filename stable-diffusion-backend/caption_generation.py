# -*- coding: utf-8 -*-
"""Copy of Captions_Jun

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UlW3xZzfiQlKHvwAAXYd1TEXprQpOPBd
"""

!pip install praw nltk gensim pyLDAvis
!pip install praw
!pip install transformers torch
!pip install transformers
!pip install pinecone
!pip install torch
!pip install pillow
!pip install transformers accelerate torch


PINECONE_API_KEY="b934dced-cd63-4caf-b049-a2d340d61108"
import pinecone

import os
from pinecone import Pinecone, ServerlessSpec

pc = Pinecone(api_key="b934dced-cd63-4caf-b049-a2d340d61108")
!pip install sentence-transformers

from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM


from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch

import praw
import pandas as pd
import nltk
import re
import string
import warnings

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize


from huggingface_hub import login
login('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')
import pprint

warnings.filterwarnings('ignore')
nltk.download('stopwords')
nltk.download('wordnet')
REDDIT_CLIENT_ID = 'SW_g1anqs3P9PLmd6qy0dg'
REDDIT_CLIENT_SECRET = 'xm2jmFLj2PgJkBZJfm96QlV12b1ZlQ'
REDDIT_USER_AGENT = 'Elegant_Link5856'
reddit = praw.Reddit(client_id=REDDIT_CLIENT_ID,
                     client_secret=REDDIT_CLIENT_SECRET,
                     user_agent=REDDIT_USER_AGENT)

# Define the subreddit and keyword
subreddit_name = 'cars'  # Change to a relevant subreddit
keyword = 'bmw m5'       # Replace with your product or brand

post_limit = 100
posts_data = []
def extract_posts(subreddit_name, keyword, post_limit):
    subreddit = reddit.subreddit(subreddit_name)
    for submission in subreddit.search(keyword, limit=post_limit):
        post_details = {
            'title': submission.title,
            'selftext': submission.selftext,
            'combined_text': submission.title + ' ' + submission.selftext
        }
        posts_data.append(post_details)
    return pd.DataFrame(posts_data)

df = extract_posts(subreddit_name, keyword, post_limit)


def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = text.strip()
    return text

df['clean_text'] = df['combined_text'].apply(preprocess_text)

pinecone_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

df['embedding'] = df['clean_text'].apply(lambda x: pinecone_model.encode(x))
df['id'] = (range(1, len(df) + 1))
df['id'] = (df['id']).astype('string')

pc.delete_index("reddit-posts-index")

pc = Pinecone(api_key='dabcedf0-0f88-4298-9943-d08c85acf3c2')

index_name = "reddit-posts-index"

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=384,
        metric='cosine',
        spec=ServerlessSpec(
            cloud='aws',
            region='us-east-1'
        )
    )


index = pc.Index(index_name)

for i, row in df.iterrows():
    index.upsert(vectors=[(row['id'], row['embedding'])])

query = "weight"
query_embedding = pinecone_model.encode(query).tolist()
results = index.query(vector=[query_embedding], top_k=5)

data = []

for match in results['matches']:
    post_id = match['id']
    score = match['score']
    post_text = df.loc[df['id'] == post_id, 'clean_text'].values[0]
    data.append({
        'post_id': post_id,
        'score': score,
        'post_text': post_text
    })

results_df = pd.DataFrame(data)

body = (' '.join(results_df['post_text'].dropna()))

peg_model_name = "google/pegasus-xsum"
peg_tokenizer = AutoTokenizer.from_pretrained(peg_model_name)
peg_model = AutoModelForSeq2SeqLM.from_pretrained(peg_model_name)

inputs = peg_tokenizer(body, max_length=512, truncation=True, return_tensors="pt")
summary_ids = peg_model.generate(inputs["input_ids"], max_length=30, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)

peg_summary = peg_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
peg_summary

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")

base_image = ##path_to_image

image = Image.open(base_image)

blip_inputs = processor(image, return_tensors="pt")
blip_outputs = blip_model.generate(**blip_inputs)

blip_summary = (processor.decode(blip_outputs[0], skip_special_tokens=True))
blip_summary

import re

def remove_text_between_tags(text, start_tag, end_tag):
  pattern = fr'{start_tag}(.|\n)*?{end_tag}'
  cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)
  return cleaned_text
def remove_final_tag(text, end_tag):
  pattern = fr'\s?{end_tag}'
  cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)
  return cleaned_text
def ret_post_final_tag(text, end_tag):
  cleaned_text = text.split(end_tag)[-1]
  return cleaned_text
def remove_after_last_curlybrace(string):
  last_brace_index = string.rfind('}')
  if last_brace_index != -1 and last_brace_index != len(string) - 1:
    string = string[:last_brace_index + 1]
  return string

start = "<\|begin_of_text\|>"
fin = "assistant<\|end_header_id\|>\n\n"
fin2 = "<\|eot_id\|>"


!pip uninstall -y transformers
!pip install -q -U transformers
!pip install -q accelerate
!pip install -q bitsandbytes

import torch

device = "cuda:0" if torch.cuda.is_available() else "cpu"
model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"

import pprint
device = "cuda:0" if torch.cuda.is_available() else "cpu"
llama_model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig


nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)

!pip install -U bitsandbytes
device = "cuda"
llama_model = AutoModelForCausalLM.from_pretrained(llama_model_id, quantization_config=nf4_config)
llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_id)

myprompt = ("generate a very short marketing caption using these two inputs to show the positives:", peg_summary + blip_summary)
messages = [
    {"role": "user", "content": myprompt}
]

encodeds = llama_tokenizer.apply_chat_template(messages, return_tensors="pt").to(device)


generated_ids = llama_model.generate(encodeds, max_new_tokens=1000, do_sample=True, pad_token_id=llama_tokenizer.eos_token_id)
decoded = llama_tokenizer.batch_decode(generated_ids)
blurb = decoded[0]
cleaned1 = remove_text_between_tags(blurb, start, fin)
caption = remove_final_tag(cleaned1, fin2)
final_caption = re.findall(r'"(.*?)"', caption)
final_caption
